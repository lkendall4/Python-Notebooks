{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\student\\anaconda3new\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\student\\anaconda3new\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\student\\anaconda3new\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: regex in c:\\users\\student\\anaconda3new\\lib\\site-packages (from nltk) (2020.6.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\student\\anaconda3new\\lib\\site-packages (from nltk) (4.47.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, WordNetLemmatizer\n",
    "\n",
    "\n",
    "amica_df = pd.read_csv('C:/Users/student/Desktop/amica_dataset.csv')\n",
    "liberty_df = pd.read_csv('C:/Users/student/Desktop/liberty_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have added the code above as a way of dealing with writing over data frames as we clean our data. Anytime we want to process a data frame with some cleaning algorithm, we may not care about the old data and so will want to simply write-over the old data frame. Although it is not always a best practice to write over old data with new data, it is often more efficient for memory and so I simply suggest using your own discretion. Using the above code will stop Pandas from printing a warning to this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>User Creation Date</th>\n",
       "      <th>gender</th>\n",
       "      <th>Tweet Timestamp</th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>retweeted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>989465942500528128</td>\n",
       "      <td>Tim Clarke</td>\n",
       "      <td>timyclarke</td>\n",
       "      <td>Fort Lauderdale, FL</td>\n",
       "      <td>A daydreamer with unending quest to find what ...</td>\n",
       "      <td>92</td>\n",
       "      <td>662</td>\n",
       "      <td>237</td>\n",
       "      <td>2018-04-26 11:27:18+00:00</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>158562722</td>\n",
       "      <td>Edge Pusher</td>\n",
       "      <td>edgepusher</td>\n",
       "      <td>In the screwed up US of A</td>\n",
       "      <td>Programmer, writer, mother, musician. Wonders ...</td>\n",
       "      <td>930</td>\n",
       "      <td>2078</td>\n",
       "      <td>2002</td>\n",
       "      <td>2010-06-23 01:23:45+00:00</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Tue Nov 03 02:34:08 +0000 2020</td>\n",
       "      <td>1.323453e+18</td>\n",
       "      <td>@LCarrington259 @rcmahoney @JoeBiden I think i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>158562722</td>\n",
       "      <td>Edge Pusher</td>\n",
       "      <td>edgepusher</td>\n",
       "      <td>In the screwed up US of A</td>\n",
       "      <td>Programmer, writer, mother, musician. Wonders ...</td>\n",
       "      <td>930</td>\n",
       "      <td>2078</td>\n",
       "      <td>2002</td>\n",
       "      <td>2010-06-23 01:23:45+00:00</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Mon Nov 02 20:20:43 +0000 2020</td>\n",
       "      <td>1.323359e+18</td>\n",
       "      <td>@GeorgeTakei Homosexuals overturn our savior D...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>158562722</td>\n",
       "      <td>Edge Pusher</td>\n",
       "      <td>edgepusher</td>\n",
       "      <td>In the screwed up US of A</td>\n",
       "      <td>Programmer, writer, mother, musician. Wonders ...</td>\n",
       "      <td>930</td>\n",
       "      <td>2078</td>\n",
       "      <td>2002</td>\n",
       "      <td>2010-06-23 01:23:45+00:00</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Mon Nov 02 18:45:27 +0000 2020</td>\n",
       "      <td>1.323335e+18</td>\n",
       "      <td>@Matthew02388332 @drileyelird @PeteButtigieg A...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>158562722</td>\n",
       "      <td>Edge Pusher</td>\n",
       "      <td>edgepusher</td>\n",
       "      <td>In the screwed up US of A</td>\n",
       "      <td>Programmer, writer, mother, musician. Wonders ...</td>\n",
       "      <td>930</td>\n",
       "      <td>2078</td>\n",
       "      <td>2002</td>\n",
       "      <td>2010-06-23 01:23:45+00:00</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Tue Nov 03 02:34:08 +0000 2020</td>\n",
       "      <td>1.323453e+18</td>\n",
       "      <td>@LCarrington259 @rcmahoney @JoeBiden I think i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              User ID         name screen_name                   location  \\\n",
       "0  989465942500528128   Tim Clarke  timyclarke        Fort Lauderdale, FL   \n",
       "1           158562722  Edge Pusher  edgepusher  In the screwed up US of A   \n",
       "2           158562722  Edge Pusher  edgepusher  In the screwed up US of A   \n",
       "3           158562722  Edge Pusher  edgepusher  In the screwed up US of A   \n",
       "4           158562722  Edge Pusher  edgepusher  In the screwed up US of A   \n",
       "\n",
       "                                         description  followers_count  \\\n",
       "0  A daydreamer with unending quest to find what ...               92   \n",
       "1  Programmer, writer, mother, musician. Wonders ...              930   \n",
       "2  Programmer, writer, mother, musician. Wonders ...              930   \n",
       "3  Programmer, writer, mother, musician. Wonders ...              930   \n",
       "4  Programmer, writer, mother, musician. Wonders ...              930   \n",
       "\n",
       "   friends_count  favourites_count         User Creation Date   gender  \\\n",
       "0            662               237  2018-04-26 11:27:18+00:00     male   \n",
       "1           2078              2002  2010-06-23 01:23:45+00:00  unknown   \n",
       "2           2078              2002  2010-06-23 01:23:45+00:00  unknown   \n",
       "3           2078              2002  2010-06-23 01:23:45+00:00  unknown   \n",
       "4           2078              2002  2010-06-23 01:23:45+00:00  unknown   \n",
       "\n",
       "                  Tweet Timestamp      Tweet ID  \\\n",
       "0                             NaN           NaN   \n",
       "1  Tue Nov 03 02:34:08 +0000 2020  1.323453e+18   \n",
       "2  Mon Nov 02 20:20:43 +0000 2020  1.323359e+18   \n",
       "3  Mon Nov 02 18:45:27 +0000 2020  1.323335e+18   \n",
       "4  Tue Nov 03 02:34:08 +0000 2020  1.323453e+18   \n",
       "\n",
       "                                                text  retweet_count  \\\n",
       "0                                                NaN            NaN   \n",
       "1  @LCarrington259 @rcmahoney @JoeBiden I think i...            0.0   \n",
       "2  @GeorgeTakei Homosexuals overturn our savior D...            0.0   \n",
       "3  @Matthew02388332 @drileyelird @PeteButtigieg A...            0.0   \n",
       "4  @LCarrington259 @rcmahoney @JoeBiden I think i...            0.0   \n",
       "\n",
       "   favorite_count is_quote_status retweeted  \n",
       "0             NaN             NaN       NaN  \n",
       "1            20.0           False     False  \n",
       "2             1.0           False     False  \n",
       "3             0.0           False     False  \n",
       "4            20.0           False     False  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amica_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = 'text'\n",
    "group_col = 'gender'\n",
    "adf_text = amica_df[[group_col, text_col]]\n",
    "\n",
    "adf_text[text_col] = adf_text[text_col].replace(to_replace=r'[ , | ? | $ | . | ! | - | : ]' , value = r'', regex = True)\n",
    "adf_text[text_col] = adf_text[text_col].replace(to_replace=r'[ ^a-zA-Z ] ', value = r' ', regex = True)\n",
    "adf_text[text_col] = adf_text[text_col].replace(to_replace=r'\\s\\s+' , value = r' ', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldf_text = liberty_df[[group_col, text_col]]\n",
    "\n",
    "ldf_text[text_col] = ldf_text[text_col].replace(to_replace=r'[ , | ? | $ | . | ! | - | : ]' , value = r'', regex = True)\n",
    "#ldf_text[text_col] = ldf_text[text_col].replace(to_replace=r'[ ^a-zA-Z ] ', value = r' ', regex = True)\n",
    "ldf_text[text_col] = ldf_text[text_col].replace(to_replace=r'\\s\\s+' , value = r' ', regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next set of code we first identify the column that contains our grouper like gender. We then identify our column that contains our text data. Once we have identified our two most important columns we create a new data frame of just those columns called df_text.\n",
    "\n",
    "Finally, I have included 3 different sets of code for doing some initial processing of the text data using Regex functions. The first function replaces funny symbols with nothing in order to remove funny symbols from analysis. You can add more symbols that may be unique to your data set by adding a | and then the symbol after. The second regex function replaces all non-letters with a space. The last regex pattern removes extra blank spaces and replaces them with a single space to ensure that each word only contains one space to the next word. These obviously have overlapping effects so use one, all, or modify to your specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unknown</td>\n",
       "      <td>@LCarrington259 @rcmahone @JoeBide thin it’ si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unknown</td>\n",
       "      <td>@GeorgeTake Homosexual overtur ou savio Donal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unknown</td>\n",
       "      <td>@Matthew02388332 @drileyelir @PeteButtigie Als...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>unknown</td>\n",
       "      <td>@LCarrington259 @rcmahone @JoeBide thin it’ si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>unknown</td>\n",
       "      <td>@GeorgeTake Homosexual overtur ou savio Donal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unknown</td>\n",
       "      <td>@Matthew02388332 @drileyelir @PeteButtigie Als...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mostly_male</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        gender                                               text\n",
       "0         male                                                NaN\n",
       "1      unknown  @LCarrington259 @rcmahone @JoeBide thin it’ si...\n",
       "2      unknown  @GeorgeTake Homosexual overtur ou savio Donal ...\n",
       "3      unknown  @Matthew02388332 @drileyelir @PeteButtigie Als...\n",
       "4      unknown  @LCarrington259 @rcmahone @JoeBide thin it’ si...\n",
       "5      unknown  @GeorgeTake Homosexual overtur ou savio Donal ...\n",
       "6      unknown  @Matthew02388332 @drileyelir @PeteButtigie Als...\n",
       "7      unknown                                                NaN\n",
       "8  mostly_male                                                NaN\n",
       "9      unknown                                                NaN"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adf_text.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error in data type shown below. We must convert the text column to a string type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@GeorgeTake Homosexual overtur ou savio Donal Trum 😆 He' probabl enjo i \""
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adf_text.text[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= [adf_text, ldf_text]\n",
    "for df in text:\n",
    "    df['text'] = df['text'].astype(str)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text= [adf_text, ldf_text]\n",
    "for df in text:\n",
    "    df['text'] = df['text'].map( {'none': NaN}).astype(str)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text= [adf_text, ldf_text]\n",
    "for df in text:\n",
    "    df['text']=df['text'].dropna().inplace=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "operators = set(['not','n/a','na'])\n",
    "stopwords = stop - operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokens, stopwords):\n",
    "    return [token for token in tokens if token not in stopwords]\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    if treebank_tag.startswith('V'):\n",
    "           return wordnet.VERB\n",
    "    if treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    if treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "def lemmarati(tup_list):\n",
    "    if not (np.all(pd.notnull(tup_list))):\n",
    "        return tup_list\n",
    "    outputlist = []\n",
    "    for i, j in tup_list:\n",
    "        pos = get_wordnet_pos(i)\n",
    "        lemma = wnl.lemmatize(i)\n",
    "        outputlist.append(lemma)\n",
    "    return outputlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next set of code, we are activating and setting up some functions that will allow us to do some more cleaning and normalizing of the text data. More specifically, the code sets up a function to remove stopwords, or words that are very common and as a result not all that meaningful (e.g. the). The remaining code also performs lemmatization. Lemmatization is a way of normalizing text so that words like Python, Pythons, and Pythonic all become just Python. Thus, lemmatization is like stemming but it takes the part of speech into account so that meet (v) and meeting (n) are kept separate.\n",
    "\n",
    "Also, note that before defining our stopword list we remove some words that we want to keep in our topic analysis. Words like ‘not’ although often considered a stopword, can be very important when performing topic or sentiment analysis. Consider the difference between ‘happy’ and ‘not happy.’ The latter is the opposite of the former however if we used the nltk stopwords list we would remove ‘not’ from the list and run the risk of thinking most comments were ‘happy’ when in reality they were ‘not happy.’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text = [adf_text, ldf_text]\n",
    "for df in text:\n",
    "    df[text_col] = df[text_col].map(lambda x: nltk.word_tokenize(x.lower()) if (np.all(pd.notnull(x))) else x.lower())\n",
    "\n",
    "    df[text_col] = df[text_col].map(lambda x: pos_tag(x) if (np.all(pd.notnull(x))) else x)\n",
    "\n",
    "    df[text_col] = df[text_col].map(lemmarati)\n",
    "\n",
    "    df[text_col] = df[text_col].map(lambda x: remove_stopwords(x,stopwords) if (np.all(pd.notnull(x))) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "adf_text[text_col] = adf_text[text_col].map(lambda x: nltk.word_tokenize(x.lower()) if (np.all(pd.notnull(x))) else x.lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adf_text[text_col] = adf_text[text_col].map(lambda x: pos_tag(x) if (np.all(pd.notnull(x))) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_text[text_col] = adf_text[text_col].map(lemmarati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_text[text_col] = adf_text[text_col].map(lambda x: remove_stopwords(x,stopwords) if (np.all(pd.notnull(x))) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_text[text_col] = adf_text[text_col].map(lambda x: ' '.join(x) if (np.all(pd.notnull(x))) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "n_features = 1000\n",
    "n_topics= 10\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(max_df = .95, min_df = 2, max_features = n_features, ngram_range = (2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\student\\anaconda3new\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1076: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "C:\\Users\\student\\anaconda3new\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1076: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n",
      "C:\\Users\\student\\anaconda3new\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1076: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    }
   ],
   "source": [
    "groups = df_text[group_col].unique()\n",
    "results = []\n",
    "\n",
    "for i in groups: \n",
    "    df_grp = adf_text.loc[adf_text[group_col] == i]\n",
    "    if len(df_grp[text_col]) > 3:\n",
    "        tf = tfidf_vec.fit_transform(df_grp[text_col])\n",
    "        feature_names = tfidf_vec.get_feature_names()\n",
    "        try:\n",
    "            nmf = NMF(n_components = n_topics, random_state=1,alpha=.1, l1_ratio=.5).fit(tf)\n",
    "            df_topics = pd.DataFrame(nmf.components_)\n",
    "            df_topics.columns = feature_names\n",
    "            df_top = df_topics.apply(lambda x: pd.Series(x.sort_values(ascending=False).iloc[:5].index,index=['top1','top2','top3','top4','top5']), axis=1).reset_index()\n",
    "            df_top['Group'] = i\n",
    "            results.append(df_top)\n",
    "        except:\n",
    "            results.append(i+' Did not produce topic results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we first get a list of the unique groups in our grouping column. We then create a container (in this case a list) to hold our resulting data frames from the NMF topic analysis.\n",
    "\n",
    "In the for loop, we perform a separate NMF analysis for each unique group contained in the grouping column. We use the ‘if len(df_grp[text_col]) > 100’ logic to ensure we have enough rows of text for the analysis. We use the ‘try:’ statement to ensure that the analysis will still run in case one of the groups gives us an error. In the ‘try:’ code we perform the NMF, extract the components into a data frame, label the data frame with the feature names (the bi and trigrams), selecting only the top 5 bi and trigrams for each topic based on their numeric contribution to the topic, add a column to the data frame to keep track of which group the topics are for, and append the results into our results list.\n",
    "\n",
    "Now we have a list of data frames, which are not useful as a list so one more step before we finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_results = pd.concat(results,axis=0)\n",
    "topic_results.to_csv('C:/Users/student/Desktop/my_aNMF_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>top1</th>\n",
       "      <th>top2</th>\n",
       "      <th>top3</th>\n",
       "      <th>top4</th>\n",
       "      <th>top5</th>\n",
       "      <th>Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https tco</td>\n",
       "      <td>https tco rc5z3gfeau</td>\n",
       "      <td>tco rc5z3gfeau</td>\n",
       "      <td>makemoneyonline https</td>\n",
       "      <td>makemoneyonline https tco</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>realdonaldtrump joebiden</td>\n",
       "      <td>patricksvitek dccc realdonaldtrump</td>\n",
       "      <td>suezq82cubfan patricksvitek</td>\n",
       "      <td>joebiden brandyfortexas senkamalaharris</td>\n",
       "      <td>joebiden brandyfortexas</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>supermarioodyssey nintendoswitch</td>\n",
       "      <td>zrgsafg58avia klowdtelevision</td>\n",
       "      <td>joebiden brandyfortexas senkamalaharris</td>\n",
       "      <td>italiancharitiesofamericaawarded https tco</td>\n",
       "      <td>italianhorrorcinemathemostinfluentialhorrorfil...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tco dobuyktkpb</td>\n",
       "      <td>zrgsafg58avia klowdtelevision</td>\n",
       "      <td>italiancharitiesofamericaawarded https</td>\n",
       "      <td>italianhorrorcinemathemostinfluentialhorrorfil...</td>\n",
       "      <td>italianhorrorcinemathemostinfluentialhorrorfil...</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>rt berniesanders</td>\n",
       "      <td>zrgsafg58avia klowdtelevision</td>\n",
       "      <td>italiancharitiesofamericaawarded https tco</td>\n",
       "      <td>italianhorrorcinemathemostinfluentialhorrorfil...</td>\n",
       "      <td>ithoughtyou dbeinterestedinthisstory</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                              top1  \\\n",
       "0      0                         https tco   \n",
       "1      1          realdonaldtrump joebiden   \n",
       "2      2  supermarioodyssey nintendoswitch   \n",
       "3      3                    tco dobuyktkpb   \n",
       "4      4                  rt berniesanders   \n",
       "\n",
       "                                 top2  \\\n",
       "0                https tco rc5z3gfeau   \n",
       "1  patricksvitek dccc realdonaldtrump   \n",
       "2       zrgsafg58avia klowdtelevision   \n",
       "3       zrgsafg58avia klowdtelevision   \n",
       "4       zrgsafg58avia klowdtelevision   \n",
       "\n",
       "                                         top3  \\\n",
       "0                              tco rc5z3gfeau   \n",
       "1                 suezq82cubfan patricksvitek   \n",
       "2     joebiden brandyfortexas senkamalaharris   \n",
       "3      italiancharitiesofamericaawarded https   \n",
       "4  italiancharitiesofamericaawarded https tco   \n",
       "\n",
       "                                                top4  \\\n",
       "0                              makemoneyonline https   \n",
       "1            joebiden brandyfortexas senkamalaharris   \n",
       "2         italiancharitiesofamericaawarded https tco   \n",
       "3  italianhorrorcinemathemostinfluentialhorrorfil...   \n",
       "4  italianhorrorcinemathemostinfluentialhorrorfil...   \n",
       "\n",
       "                                                top5 Group  \n",
       "0                          makemoneyonline https tco  male  \n",
       "1                            joebiden brandyfortexas  male  \n",
       "2  italianhorrorcinemathemostinfluentialhorrorfil...  male  \n",
       "3  italianhorrorcinemathemostinfluentialhorrorfil...  male  \n",
       "4               ithoughtyou dbeinterestedinthisstory  male  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4      italiancharitiesofamericaawarded https tco\n",
       "4                    whataboutyou haveyouvotedyet\n",
       "4               carey fratalano matteo_marzolican\n",
       "4        grainnekelly5 bubblebumukltd theampderry\n",
       "4    freethoughtkaty quantumentang13 shomaristone\n",
       "4                                  tco 0rqn7en3av\n",
       "Name: top3, dtype: object"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_results.top3[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
